----------------------------------------------------------------
   Matrix Factorization Feed-forward Neural Network -> MLP
----------------------------------------------------------------
Persentase data latih: 90.00%
Persentase data uji  : 10.00%


Hyperparameter Matrix Factorization:
Latent factors / Dimensi laten: 128
Learning rate                 : 0.005
Regularization parameter      : 0.03
Jumlah epoch / training       : 88

Hyperparameter MLP:
Struktur Hidden Layer     : [64, 32, 16, 8]
Learning Rate             : 0.001
Batch Size                : 32
Jumlah epoch / training   : 30
Early Stopping (Patience) : 10

Epoch 1/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 3.1478 - val_loss: 0.9407
Epoch 2/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.8931 - val_loss: 0.9064
Epoch 3/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.8773 - val_loss: 0.8873
Epoch 4/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.8727 - val_loss: 0.8795
Epoch 5/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.8466 - val_loss: 0.8638
Epoch 6/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.8364 - val_loss: 0.8527
Epoch 7/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.8185 - val_loss: 0.8422
Epoch 8/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.8100 - val_loss: 0.8263
Epoch 9/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7966 - val_loss: 0.8216
Epoch 10/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7934 - val_loss: 0.8055
Epoch 11/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7734 - val_loss: 0.8110
Epoch 12/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7776 - val_loss: 0.7872
Epoch 13/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7663 - val_loss: 0.7947
Epoch 14/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7426 - val_loss: 0.7752
Epoch 15/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7395 - val_loss: 0.7788
Epoch 16/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7415 - val_loss: 0.7668
Epoch 17/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7242 - val_loss: 0.7647
Epoch 18/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7211 - val_loss: 0.7537
Epoch 19/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7186 - val_loss: 0.7501
Epoch 20/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7194 - val_loss: 0.7498
Epoch 21/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7105 - val_loss: 0.7464
Epoch 22/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.7047 - val_loss: 0.7399
Epoch 23/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6908 - val_loss: 0.7380
Epoch 24/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6964 - val_loss: 0.7371
Epoch 25/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6873 - val_loss: 0.7434
Epoch 26/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6972 - val_loss: 0.7271
Epoch 27/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6872 - val_loss: 0.7338
Epoch 28/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6781 - val_loss: 0.7281
Epoch 29/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6736 - val_loss: 0.7287
Epoch 30/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6832 - val_loss: 0.7258
Epoch 31/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6752 - val_loss: 0.7201
Epoch 32/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6698 - val_loss: 0.7179
Epoch 33/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6619 - val_loss: 0.7169
Epoch 34/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6652 - val_loss: 0.7135
Epoch 35/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6558 - val_loss: 0.7226
Epoch 36/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6624 - val_loss: 0.7118
Epoch 37/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6570 - val_loss: 0.7040
Epoch 38/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6602 - val_loss: 0.7057
Epoch 39/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6497 - val_loss: 0.7086
Epoch 40/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6427 - val_loss: 0.7030
Epoch 41/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6493 - val_loss: 0.7140
Epoch 42/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6394 - val_loss: 0.6967
Epoch 43/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6390 - val_loss: 0.6954
Epoch 44/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6320 - val_loss: 0.6942
Epoch 45/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6305 - val_loss: 0.6960
Epoch 46/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6335 - val_loss: 0.6922
Epoch 47/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6286 - val_loss: 0.6889
Epoch 48/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6261 - val_loss: 0.6879
Epoch 49/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6263 - val_loss: 0.6906
Epoch 50/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6243 - val_loss: 0.6872
Epoch 51/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6254 - val_loss: 0.6868
Epoch 52/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6172 - val_loss: 0.6882
Epoch 53/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6182 - val_loss: 0.6808
Epoch 54/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6077 - val_loss: 0.6855
Epoch 55/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6167 - val_loss: 0.6850
Epoch 56/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6041 - val_loss: 0.6907
Epoch 57/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6027 - val_loss: 0.6854
Epoch 58/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6013 - val_loss: 0.6751
Epoch 59/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6070 - val_loss: 0.6767
Epoch 60/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6056 - val_loss: 0.6766
Epoch 61/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5993 - val_loss: 0.6752
Epoch 62/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6037 - val_loss: 0.6751
Epoch 63/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5985 - val_loss: 0.6761
Epoch 64/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5887 - val_loss: 0.6782
Epoch 65/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5944 - val_loss: 0.6784
Epoch 66/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.6030 - val_loss: 0.6692
Epoch 67/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5943 - val_loss: 0.6785
Epoch 68/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5862 - val_loss: 0.6739
Epoch 69/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5850 - val_loss: 0.6775
Epoch 70/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5869 - val_loss: 0.6726
Epoch 71/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5836 - val_loss: 0.6768
Epoch 72/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5795 - val_loss: 0.6726
Epoch 73/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5857 - val_loss: 0.6688
Epoch 74/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5721 - val_loss: 0.6726
Epoch 75/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5786 - val_loss: 0.6756
Epoch 76/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5776 - val_loss: 0.6689
Epoch 77/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5760 - val_loss: 0.6728
Epoch 78/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5702 - val_loss: 0.6734
Epoch 79/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5817 - val_loss: 0.6776
Epoch 80/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5697 - val_loss: 0.6743
Epoch 81/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5705 - val_loss: 0.6708
Epoch 82/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5627 - val_loss: 0.6632
Epoch 83/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5663 - val_loss: 0.6774
Epoch 84/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5685 - val_loss: 0.6721
Epoch 85/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5605 - val_loss: 0.6686
Epoch 86/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5625 - val_loss: 0.6711
Epoch 87/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5618 - val_loss: 0.6660
Epoch 88/88
284/284 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 0.5609 - val_loss: 0.6725
185707/185707 ━━━━━━━━━━━━━━━━━━━━ 104s 560us/step

📊 Evaluasi Model MLP (MF + feature + Swish):
MAE : 0.6048
MSE : 0.6842
RMSE: 0.8271

Total kombinasi user-item diuji : 5942620
Diproses oleh model            : 5942620
Memiliki rating aktual         : 100836
         userId  itemId  actual_rating  ffnn_predicted_rating
0             1       1            4.0                    3.9
1             1       2            0.0                    2.9
2             1       3            4.0                    3.6
3             1       4            0.0                    2.9
4             1       5            0.0                    3.0
...         ...     ...            ...                    ...
5942615     610  193581            0.0                    0.2
5942616     610  193583            0.0                    0.4
5942617     610  193585            0.0                    0.1
5942618     610  193587            0.0                    0.3
5942619     610  193609            0.0                    0.2

[5942620 rows x 4 columns]

📁 Hasil prediksi disimpan ke: b_ffnn_ratings.csv
⏱️ Waktu yang dibutuhkan: 11617.97 detik