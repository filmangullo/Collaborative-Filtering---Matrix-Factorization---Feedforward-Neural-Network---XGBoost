||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
                              MATRIX FACTORIZATION
                                      and
           FEEDFORWARD NEURAL NETWORK BASED ON MULTI-LAYER PERCEPTRON
||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
--------------------------------------------------------------------------------
                                   LOAD DATA
--------------------------------------------------------------------------------
Total Item: 6
Total Rating  : 27
--------------------------------------------------------------------------------
                                 DATA SPLITTING
--------------------------------------------------------------------------------
Persentase data latih: 88.89%
Persentase data uji  : 11.11%


--------------------------------------------------------------------------------
                                DATA PREPARATION
--------------------------------------------------------------------------------
========== ONE-HOT ENCODED FEATURES ==========
   id  Adventure  Animation  Children  Comedy  Drama  Fantasy  Romance
0   1          1          1         1       1      0        1        0
1   2          1          0         1       0      0        1        0
2   3          0          0         0       1      0        0        1
3   4          0          0         0       1      1        0        1
4   5          0          0         0       1      0        0        0
5   8          1          1         1       0      0        0        0

Dimensi data akhir (item + fitur): (6, 8)


========== USER-ITEM RATING MATRIX (PIVOT TABLE)  ==========
itemId    1    2    3    4    5   8
userId
11      5.0  4.0  5.0  4.0  4.0 NaN
12      4.0  2.0  NaN  NaN  NaN NaN
13      5.0  3.0  NaN  NaN  NaN NaN
14      NaN  NaN  NaN  4.0  3.0 NaN
15      4.0  NaN  5.0  NaN  5.0 NaN
16      NaN  NaN  NaN  5.0  5.0 NaN
17      NaN  NaN  NaN  NaN  5.0 NaN
18      5.0  NaN  NaN  4.0  NaN NaN
88      NaN  NaN  NaN  NaN  NaN NaN
99      NaN  NaN  NaN  NaN  NaN NaN
100     NaN  NaN  NaN  NaN  NaN NaN


Hyperparameter Matrix Factorization:
Latent factors / Dimensi laten: 64
Learning rate                 : 0.01
Regularization parameter      : 0.01
Jumlah epoch / training       : 40


Hyperparameter MLP:
Struktur Hidden Layer     : [256, 128, 64, 32, 16, 8]
Learning Rate             : 0.005
Batch Size                : 256
Jumlah epoch / training   : 55
Early Stopping (Patience) : 15


Epoch 1/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m2s←[0m 2s/step - loss: 18.5411 - val_loss: 14.0529
Epoch 2/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 107ms/step - loss: 14.4036 - val_loss: 2.3289
Epoch 3/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 102ms/step - loss: 2.5802 - val_loss: 29.3740
Epoch 4/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 105ms/step - loss: 30.0507 - val_loss: 1.0940
Epoch 5/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 104ms/step - loss: 1.5047 - val_loss: 2.7683
Epoch 6/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 105ms/step - loss: 3.0689 - val_loss: 6.7548
Epoch 7/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 103ms/step - loss: 7.0877 - val_loss: 8.3714
Epoch 8/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 106ms/step - loss: 8.7351 - val_loss: 8.5186
Epoch 9/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 101ms/step - loss: 8.8975 - val_loss: 7.7645
Epoch 10/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 101ms/step - loss: 8.1506 - val_loss: 6.3824
Epoch 11/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 102ms/step - loss: 6.7710 - val_loss: 4.5682
Epoch 12/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 102ms/step - loss: 4.9545 - val_loss: 2.5766
Epoch 13/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 103ms/step - loss: 2.9529 - val_loss: 0.8520
Epoch 14/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 103ms/step - loss: 1.2078 - val_loss: 0.1321
Epoch 15/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 104ms/step - loss: 0.4584 - val_loss: 1.1004
Epoch 16/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 102ms/step - loss: 1.4061 - val_loss: 2.0911
Epoch 17/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 102ms/step - loss: 2.4086 - val_loss: 1.5380
Epoch 18/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 103ms/step - loss: 1.8791 - val_loss: 0.5683
Epoch 19/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 102ms/step - loss: 0.9320 - val_loss: 0.1383
Epoch 20/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 104ms/step - loss: 0.5232 - val_loss: 0.2321
Epoch 21/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 103ms/step - loss: 0.6350 - val_loss: 0.5162
Epoch 22/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 104ms/step - loss: 0.9323 - val_loss: 0.7556
Epoch 23/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 104ms/step - loss: 1.1793 - val_loss: 0.8553
Epoch 24/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 103ms/step - loss: 1.2819 - val_loss: 0.8066
Epoch 25/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 118ms/step - loss: 1.2322 - val_loss: 0.6452
Epoch 26/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 105ms/step - loss: 1.0671 - val_loss: 0.4309
Epoch 27/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 102ms/step - loss: 0.8479 - val_loss: 0.2350
Epoch 28/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 104ms/step - loss: 0.6482 - val_loss: 0.1250
Epoch 29/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 99ms/step - loss: 0.5377 - val_loss: 0.1377
Epoch 30/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 106ms/step - loss: 0.5553 - val_loss: 0.2471
Epoch 31/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 110ms/step - loss: 0.6737 - val_loss: 0.3598
Epoch 32/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 105ms/step - loss: 0.7949 - val_loss: 0.3815
Epoch 33/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 105ms/step - loss: 0.8188 - val_loss: 0.3033
Epoch 34/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 103ms/step - loss: 0.7353 - val_loss: 0.1934
Epoch 35/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 107ms/step - loss: 0.6160 - val_loss: 0.1205
Epoch 36/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 104ms/step - loss: 0.5341 - val_loss: 0.1082
Epoch 37/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 102ms/step - loss: 0.5149 - val_loss: 0.1410
Epoch 38/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 101ms/step - loss: 0.5430 - val_loss: 0.1888
Epoch 39/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 109ms/step - loss: 0.5876 - val_loss: 0.2254
Epoch 40/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 99ms/step - loss: 0.6213 - val_loss: 0.2357
Epoch 41/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 100ms/step - loss: 0.6286 - val_loss: 0.2174
Epoch 42/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 100ms/step - loss: 0.6073 - val_loss: 0.1791
Epoch 43/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 103ms/step - loss: 0.5664 - val_loss: 0.1362
Epoch 44/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 103ms/step - loss: 0.5217 - val_loss: 0.1045
Epoch 45/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 103ms/step - loss: 0.4900 - val_loss: 0.0941
Epoch 46/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 101ms/step - loss: 0.4814 - val_loss: 0.1036
Epoch 47/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 102ms/step - loss: 0.4938 - val_loss: 0.1203
Epoch 48/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 101ms/step - loss: 0.5127 - val_loss: 0.1281
Epoch 49/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 102ms/step - loss: 0.5201 - val_loss: 0.1194
Epoch 50/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 102ms/step - loss: 0.5072 - val_loss: 0.1002
Epoch 51/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 103ms/step - loss: 0.4803 - val_loss: 0.0840
Epoch 52/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 106ms/step - loss: 0.4544 - val_loss: 0.0811
Epoch 53/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 103ms/step - loss: 0.4414 - val_loss: 0.0917
Epoch 54/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 102ms/step - loss: 0.4428 - val_loss: 0.1073
Epoch 55/55
←[1m1/1←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 101ms/step - loss: 0.4507 - val_loss: 0.1172
←[1m3/3←[0m ←[32m━━━━━━━━━━━━━━━━━━━━←[0m←[37m←[0m ←[1m0s←[0m 50ms/step

📊 Evaluasi Model MLP (MF + feature + Swish):
MAE : 0.4920
MSE : 0.3555
RMSE: 0.5963

Total kombinasi user-item diuji : 66
Diproses oleh model            : 66
Memiliki rating aktual         : 22
    userId  itemId  actual_rating  ffnn_predicted_rating
0       11       1            4.0                    4.0
1       11       2            4.0                    4.0
2       11       3            5.0                    4.0
3       11       4            4.0                    4.0
4       11       5            4.0                    5.0
..     ...     ...            ...                    ...
61     100       2            0.0                    3.0
62     100       3            0.0                    4.0
63     100       4            0.0                    4.0
64     100       5            0.0                    4.0
65     100       8            0.0                    2.0

[66 rows x 4 columns]

📁 Hasil prediksi disimpan ke: dataset_dummy/b_ffnn_ratings.csv
⏱️ Waktu yang dibutuhkan: 10.05 detik