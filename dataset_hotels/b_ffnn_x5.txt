||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
                              MATRIX FACTORIZATION                              
                                      and                                       
           FEEDFORWARD NEURAL NETWORK BASED ON MULTI-LAYER PERCEPTRON           
||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
--------------------------------------------------------------------------------
                                   LOAD DATA                                    
--------------------------------------------------------------------------------
Total Item: 16292
Total Rating  : 16940
--------------------------------------------------------------------------------
                                 DATA SPLITTING                                 
--------------------------------------------------------------------------------
Persentase data latih: 90.00%
Persentase data uji  : 10.00%


--------------------------------------------------------------------------------
                                DATA PREPARATION                                
--------------------------------------------------------------------------------
========== ONE-HOT ENCODED FEATURES ==========
          id  0  1  2  3  4  ...  sangkanhurip  sedati  sekotong  senggigi  sukaraja  trawas
0      69602  0  1  0  0  0  ...             0       0         0         0         0       0
1      69603  0  0  0  0  0  ...             0       0         0         0         0       0
2      69604  1  0  0  0  0  ...             0       0         0         0         0       0
3      69605  0  0  0  0  1  ...             0       0         0         0         0       0
4      69606  1  0  0  0  0  ...             0       0         0         0         0       0
...      ... .. .. .. .. ..  ...           ...     ...       ...       ...       ...     ...
16287  85889  0  0  0  1  0  ...             0       0         0         0         0       0
16288  85890  0  0  0  1  0  ...             0       0         0         0         0       0
16289  85891  0  0  0  1  0  ...             0       0         0         0         0       0
16290  85892  0  0  0  0  1  ...             0       0         0         0         0       0
16291  85893  0  0  1  0  0  ...             0       0         0         0         0       0

[16292 rows x 1252 columns]

Dimensi data akhir (item + fitur): (16292, 1252)


========== USER-ITEM RATING MATRIX (PIVOT TABLE)  ==========
itemId  69602  69603  69604  69605  69606  ...  85889  85890  85891  85892  85893
userId                                     ...                                   
1         NaN    NaN    NaN    NaN    NaN  ...    NaN    NaN    NaN    NaN    NaN
2         NaN    NaN    NaN    NaN    NaN  ...    NaN    NaN    NaN    NaN    NaN
3         NaN    NaN    NaN    NaN    NaN  ...    NaN    NaN    NaN    NaN    NaN
4         NaN    NaN    NaN    NaN    NaN  ...    NaN    NaN    NaN    NaN    NaN
5         NaN    NaN    NaN    NaN    NaN  ...    NaN    NaN    NaN    NaN    NaN
...       ...    ...    ...    ...    ...  ...    ...    ...    ...    ...    ...
8581      NaN    NaN    NaN    NaN    NaN  ...    NaN    NaN    NaN    NaN    NaN
8583      NaN    NaN    NaN    NaN    NaN  ...    NaN    NaN    NaN    NaN    NaN
8584      NaN    NaN    NaN    NaN    NaN  ...    NaN    NaN    NaN    NaN    NaN
8586      NaN    NaN    NaN    NaN    NaN  ...    NaN    NaN    NaN    NaN    NaN
8588      NaN    NaN    NaN    NaN    NaN  ...    NaN    NaN    NaN    NaN    NaN

[7056 rows x 16292 columns]


Hyperparameter Matrix Factorization:
Latent factors / Dimensi laten: 64
Learning rate                 : 0.04
Regularization parameter      : 0.03
Jumlah epoch / training       : 40


Hyperparameter MLP:
Struktur Hidden Layer     : [128, 64, 32, 16]
Learning Rate             : 0.004
Batch Size                : 64
Jumlah epoch / training   : 55
Early Stopping (Patience) : 15


Epoch 1/55
203/203 [==============================] - 1s 2ms/step - loss: 2.4858 - mae: 1.3162 - val_loss: 2.0728 - val_mae: 1.2385
Epoch 2/55
203/203 [==============================] - 0s 2ms/step - loss: 1.5957 - mae: 1.0524 - val_loss: 1.4269 - val_mae: 0.9819
Epoch 3/55
203/203 [==============================] - 0s 2ms/step - loss: 0.9991 - mae: 0.7901 - val_loss: 1.1298 - val_mae: 0.8329
Epoch 4/55
203/203 [==============================] - 0s 2ms/step - loss: 0.6604 - mae: 0.6243 - val_loss: 0.9355 - val_mae: 0.7527
Epoch 5/55
203/203 [==============================] - 0s 2ms/step - loss: 0.4542 - mae: 0.5224 - val_loss: 0.8160 - val_mae: 0.6857
Epoch 6/55
203/203 [==============================] - 0s 2ms/step - loss: 0.3402 - mae: 0.4493 - val_loss: 0.8076 - val_mae: 0.7089
Epoch 7/55
203/203 [==============================] - 0s 2ms/step - loss: 0.2786 - mae: 0.4066 - val_loss: 0.7077 - val_mae: 0.6210
Epoch 8/55
203/203 [==============================] - 0s 2ms/step - loss: 0.2239 - mae: 0.3642 - val_loss: 0.6840 - val_mae: 0.6132
Epoch 9/55
203/203 [==============================] - 0s 2ms/step - loss: 0.1939 - mae: 0.3377 - val_loss: 0.6633 - val_mae: 0.6015
Epoch 10/55
203/203 [==============================] - 0s 2ms/step - loss: 0.1585 - mae: 0.3046 - val_loss: 0.6217 - val_mae: 0.5795
Epoch 11/55
203/203 [==============================] - 0s 2ms/step - loss: 0.1349 - mae: 0.2790 - val_loss: 0.6333 - val_mae: 0.5708
Epoch 12/55
203/203 [==============================] - 0s 2ms/step - loss: 0.1209 - mae: 0.2633 - val_loss: 0.6021 - val_mae: 0.5635
Epoch 13/55
203/203 [==============================] - 0s 2ms/step - loss: 0.1076 - mae: 0.2475 - val_loss: 0.5740 - val_mae: 0.5540
Epoch 14/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0951 - mae: 0.2321 - val_loss: 0.5780 - val_mae: 0.5526
Epoch 15/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0908 - mae: 0.2268 - val_loss: 0.5944 - val_mae: 0.5463
Epoch 16/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0827 - mae: 0.2145 - val_loss: 0.5753 - val_mae: 0.5444
Epoch 17/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0742 - mae: 0.2022 - val_loss: 0.5785 - val_mae: 0.5438
Epoch 18/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0652 - mae: 0.1909 - val_loss: 0.5598 - val_mae: 0.5372
Epoch 19/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0626 - mae: 0.1851 - val_loss: 0.5689 - val_mae: 0.5346
Epoch 20/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0568 - mae: 0.1739 - val_loss: 0.5435 - val_mae: 0.5354
Epoch 21/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0540 - mae: 0.1712 - val_loss: 0.5512 - val_mae: 0.5252
Epoch 22/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0488 - mae: 0.1625 - val_loss: 0.5404 - val_mae: 0.5412
Epoch 23/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0492 - mae: 0.1619 - val_loss: 0.5413 - val_mae: 0.5537
Epoch 24/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0468 - mae: 0.1574 - val_loss: 0.5569 - val_mae: 0.5320
Epoch 25/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0430 - mae: 0.1497 - val_loss: 0.5285 - val_mae: 0.5204
Epoch 26/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0417 - mae: 0.1466 - val_loss: 0.5355 - val_mae: 0.5236
Epoch 27/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0417 - mae: 0.1481 - val_loss: 0.5221 - val_mae: 0.5130
Epoch 28/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0396 - mae: 0.1443 - val_loss: 0.5189 - val_mae: 0.5210
Epoch 29/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0413 - mae: 0.1456 - val_loss: 0.5254 - val_mae: 0.5159
Epoch 30/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0377 - mae: 0.1384 - val_loss: 0.5153 - val_mae: 0.5235
Epoch 31/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0352 - mae: 0.1342 - val_loss: 0.5179 - val_mae: 0.5107
Epoch 32/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0338 - mae: 0.1306 - val_loss: 0.5132 - val_mae: 0.5098
Epoch 33/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0428 - mae: 0.1440 - val_loss: 0.5315 - val_mae: 0.5111
Epoch 34/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0470 - mae: 0.1532 - val_loss: 0.5137 - val_mae: 0.5160
Epoch 35/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0369 - mae: 0.1334 - val_loss: 0.4966 - val_mae: 0.4990
Epoch 36/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0293 - mae: 0.1190 - val_loss: 0.5066 - val_mae: 0.4997
Epoch 37/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0276 - mae: 0.1143 - val_loss: 0.5164 - val_mae: 0.5062
Epoch 38/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0260 - mae: 0.1116 - val_loss: 0.4983 - val_mae: 0.4945
Epoch 39/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0237 - mae: 0.1067 - val_loss: 0.5077 - val_mae: 0.4946
Epoch 40/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0228 - mae: 0.1019 - val_loss: 0.5012 - val_mae: 0.4908
Epoch 41/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0223 - mae: 0.0989 - val_loss: 0.5011 - val_mae: 0.4896
Epoch 42/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0265 - mae: 0.1058 - val_loss: 0.5015 - val_mae: 0.4879
Epoch 43/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0272 - mae: 0.1083 - val_loss: 0.4997 - val_mae: 0.5059
Epoch 44/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0264 - mae: 0.1040 - val_loss: 0.5066 - val_mae: 0.4878
Epoch 45/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0328 - mae: 0.1142 - val_loss: 0.5033 - val_mae: 0.4825
Epoch 46/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0318 - mae: 0.1074 - val_loss: 0.5189 - val_mae: 0.4916
Epoch 47/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0304 - mae: 0.1060 - val_loss: 0.5080 - val_mae: 0.4865
Epoch 48/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0272 - mae: 0.0942 - val_loss: 0.5133 - val_mae: 0.4821
Epoch 49/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0312 - mae: 0.1012 - val_loss: 0.5139 - val_mae: 0.4846
Epoch 50/55
203/203 [==============================] - 0s 2ms/step - loss: 0.0248 - mae: 0.0874 - val_loss: 0.5040 - val_mae: 0.4709

📊 Evaluasi Model MLP (MF + feature + Swish):
MAE : 0.3245
MSE : 0.5713
RMSE: 0.7558


Total kombinasi user-item diuji : 114956352
Diproses oleh model            : 114956352
Memiliki rating aktual         : 16678
           userId  itemId  actual_rating  ffnn_predicted_rating
0               1   69602            0.0                    1.0
1               1   69603            0.0                    3.0
2               1   69604            0.0                    1.0
3               1   69605            0.0                    1.0
4               1   69606            0.0                    1.0
...           ...     ...            ...                    ...
114956347    8588   85889            0.0                    2.0
114956348    8588   85890            0.0                    1.0
114956349    8588   85891            0.0                    1.0
114956350    8588   85892            0.0                    1.0
114956351    8588   85893            0.0                    1.0

[114956352 rows x 4 columns]

📁 Hasil prediksi disimpan ke: dataset_hotels/b_ffnn_ratings.csv
⏱️ Waktu yang dibutuhkan: 153962.94 detik